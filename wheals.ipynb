{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eb3f5cd",
   "metadata": {},
   "source": [
    "# Assignment — Humpback Whale Identification (Kaggle) + MLflow (OSS)\n",
    "\n",
    "This notebook is **separate** from your fish notebook (big change in dataset + evaluation).\n",
    "\n",
    "Important nuance:\n",
    "- **Train with CrossEntropyLoss** (differentiable → backprop works)\n",
    "- **Evaluate with MAP@5** (ranking metric used by the Kaggle competition; not differentiable, so it’s an eval metric, not a training loss)\n",
    "\n",
    "You’ll get:\n",
    "- Kaggle competition download via `kagglehub`\n",
    "- CSV-based dataset (not ImageFolder)\n",
    "- Train/Val/Test split + optional KFold on a filtered subset (needed because many classes have too few samples)\n",
    "- MLflow logging (params, metrics, artifacts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70181ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -q install kagglehub mlflow torch torchvision pandas scikit-learn pillow tqdm matplotlib ipywidgets\n",
    "\n",
    "# If you run into permission / env issues:\n",
    "# - restart the kernel after install\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b97805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d700329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "# ---- MLflow (OSS) tracking ----\n",
    "# Option A (simple): local filesystem runs under ./mlruns\n",
    "mlflow.set_tracking_uri(\"file:\" + str(Path.cwd() / \"mlruns\"))\n",
    "\n",
    "# Option B (remote server): uncomment if you're running an MLflow server elsewhere\n",
    "# mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n",
    "\n",
    "mlflow.set_experiment(\"assignment_humpback_whale_map5\")\n",
    "print(\"MLflow tracking URI:\", mlflow.get_tracking_uri())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda285b7",
   "metadata": {},
   "source": [
    "## 1) Download the Kaggle competition dataset\n",
    "\n",
    "This uses `kagglehub.competition_download(...)`.\n",
    "\n",
    "You must have Kaggle credentials configured locally (same as usual Kaggle API usage).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98ec38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "kagglehub.login()\n",
    "\n",
    "\n",
    "comp_root = Path(kagglehub.competition_download(\n",
    "    \"humpback-whale-identification\"))\n",
    "print(\"Competition files at:\", comp_root)\n",
    "\n",
    "TRAIN_CSV = comp_root / \"train.csv\"\n",
    "TRAIN_DIR = comp_root / \"train\"\n",
    "TEST_DIR = comp_root / \"test\"\n",
    "SAMPLE_SUB = comp_root / \"sample_submission.csv\"\n",
    "\n",
    "print(\"TRAIN_CSV:\", TRAIN_CSV)\n",
    "print(\"TRAIN_DIR:\", TRAIN_DIR)\n",
    "print(\"TEST_DIR :\", TEST_DIR)\n",
    "print(\"SAMPLE_SUB:\", SAMPLE_SUB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec011562",
   "metadata": {},
   "source": [
    "## 2) Load train.csv + (important) filter classes so KFold is possible\n",
    "\n",
    "This competition has **many whale IDs with very few images** (including `new_whale`).\n",
    "Stratified KFold requires each class to have enough samples, otherwise it breaks (or is meaningless).\n",
    "\n",
    "So we do:\n",
    "- Optionally drop `new_whale`\n",
    "- Keep only classes with at least `MIN_SAMPLES_PER_CLASS` samples\n",
    "- Optionally keep only the top-N most frequent IDs (to keep training lightweight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa5151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(TRAIN_CSV)\n",
    "\n",
    "# Expected columns in train.csv for this comp are usually: Image, Id\n",
    "# We'll be defensive:\n",
    "assert {\"Image\", \"Id\"}.issubset(\n",
    "    df.columns), f\"Unexpected train.csv columns: {df.columns.tolist()}\"\n",
    "\n",
    "df[\"path\"] = df[\"Image\"].apply(lambda x: str(TRAIN_DIR / x))\n",
    "\n",
    "print(\"Raw rows:\", len(df))\n",
    "print(\"Unique IDs:\", df[\"Id\"].nunique())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6961c620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Filtering knobs (tune as needed) ----\n",
    "DROP_NEW_WHALE = True\n",
    "MIN_SAMPLES_PER_CLASS = 5   # must be >= K for KFold to be valid\n",
    "# set None to keep all filtered classes (can be huge)\n",
    "TOP_N_CLASSES = 200\n",
    "\n",
    "if DROP_NEW_WHALE:\n",
    "    df = df[df[\"Id\"] != \"new_whale\"].copy()\n",
    "\n",
    "counts = df[\"Id\"].value_counts()\n",
    "keep_ids = counts[counts >= MIN_SAMPLES_PER_CLASS].index\n",
    "df = df[df[\"Id\"].isin(keep_ids)].copy()\n",
    "\n",
    "if TOP_N_CLASSES is not None:\n",
    "    top_ids = df[\"Id\"].value_counts().head(TOP_N_CLASSES).index\n",
    "    df = df[df[\"Id\"].isin(top_ids)].copy()\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print(\"After filtering rows:\", len(df))\n",
    "print(\"After filtering unique IDs:\", df[\"Id\"].nunique())\n",
    "df[\"Id\"].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0a8507",
   "metadata": {},
   "source": [
    "## 3) Dataset + transforms\n",
    "\n",
    "We are **not** using ImageFolder (labels are in CSV). We build a small Dataset wrapper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf2d0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label mapping\n",
    "classes = sorted(df[\"Id\"].unique().tolist())\n",
    "id2idx = {c: i for i, c in enumerate(classes)}\n",
    "idx2id = {i: c for c, i in id2idx.items()}\n",
    "df[\"y\"] = df[\"Id\"].map(id2idx).astype(int)\n",
    "\n",
    "num_classes = len(classes)\n",
    "print(\"num_classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f196219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhaleDataset(Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, transform=None):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        row = self.df.iloc[i]\n",
    "        img = Image.open(row[\"path\"]).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        y = int(row[\"y\"])\n",
    "        return img, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1fb324",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=20),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "eval_tf = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6de529",
   "metadata": {},
   "source": [
    "## 4) Train/Val/Test split + loaders\n",
    "\n",
    "We make a **fixed test split** (20%), then do KFold on the remaining pool.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f8ab44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices and targets for stratification\n",
    "y_all = df[\"y\"].to_numpy()\n",
    "idx_all = np.arange(len(df))\n",
    "\n",
    "# Fixed TEST split\n",
    "sss = StratifiedShuffleSplit(\n",
    "    n_splits=1, test_size=0.2, random_state=RANDOM_SEED)\n",
    "train_pool_idx, test_idx = next(sss.split(idx_all, y_all))\n",
    "\n",
    "y_train_pool = y_all[train_pool_idx]\n",
    "\n",
    "print(\"Train pool:\", len(train_pool_idx), \"Test:\", len(test_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b249507c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loaders_from_indices(tr_idx, va_idx, te_idx, batch_size=32, num_workers=2):\n",
    "    train_ds = WhaleDataset(df.iloc[tr_idx], transform=train_tf)\n",
    "    val_ds = WhaleDataset(df.iloc[va_idx], transform=eval_tf)\n",
    "    test_ds = WhaleDataset(df.iloc[te_idx], transform=eval_tf)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=batch_size, shuffle=True,  num_workers=num_workers)\n",
    "    val_loader = DataLoader(val_ds,   batch_size=batch_size,\n",
    "                            shuffle=False, num_workers=num_workers)\n",
    "    test_loader = DataLoader(\n",
    "        test_ds,  batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8eb175",
   "metadata": {},
   "source": [
    "## 5) MAP@5 metric (competition metric)\n",
    "\n",
    "For each sample:\n",
    "- take top-5 predicted classes\n",
    "- if true class is at rank r (1..5): score = 1/r\n",
    "- else score = 0\n",
    "Then average across samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f341524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_at_k_from_probs(y_true: np.ndarray, probs: np.ndarray, k: int = 5) -> float:\n",
    "    topk = np.argsort(-probs, axis=1)[:, :k]  # (N, k)\n",
    "    scores = []\n",
    "    for i in range(len(y_true)):\n",
    "        true = y_true[i]\n",
    "        row = topk[i]\n",
    "        hit = np.where(row == true)[0]\n",
    "        if len(hit) == 0:\n",
    "            scores.append(0.0)\n",
    "        else:\n",
    "            rank = int(hit[0]) + 1  # 1-based\n",
    "            scores.append(1.0 / rank)\n",
    "    return float(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f6f7a9",
   "metadata": {},
   "source": [
    "## 6) Model: pretrained ResNet18 (simple baseline)\n",
    "\n",
    "(You can swap later to EfficientNet / DenseNet / etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dedea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_resnet18(num_classes: int):\n",
    "    weights = models.ResNet18_Weights.DEFAULT\n",
    "    model = models.resnet18(weights=weights)\n",
    "    in_f = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_f, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea14e881",
   "metadata": {},
   "source": [
    "## 7) Training/eval utilities (loss = CrossEntropy, metrics include MAP@5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95470dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, loader, criterion, optimizer=None):\n",
    "    is_train = optimizer is not None\n",
    "    model.train() if is_train else model.eval()\n",
    "\n",
    "    losses = []\n",
    "    all_probs = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.set_grad_enabled(is_train):\n",
    "        for x, y in tqdm(loader, leave=False):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            if is_train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            probs = torch.softmax(logits, dim=1).detach().cpu().numpy()\n",
    "            all_probs.append(probs)\n",
    "            all_targets.append(y.detach().cpu().numpy())\n",
    "\n",
    "    probs = np.vstack(all_probs)\n",
    "    targets = np.concatenate(all_targets)\n",
    "\n",
    "    pred = probs.argmax(axis=1)\n",
    "    acc = float(accuracy_score(targets, pred))\n",
    "    map5 = map_at_k_from_probs(targets, probs, k=5)\n",
    "\n",
    "    return float(np.mean(losses)), acc, map5, probs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02106355",
   "metadata": {},
   "source": [
    "## 8) KFold training on train_pool + MLflow logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03acd5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Settings ----\n",
    "K = 5          # set 2 for faster debugging\n",
    "EPOCHS = 2     # set 1 for faster debugging\n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "skf = StratifiedKFold(n_splits=K, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "fold_rows = []\n",
    "fold_test_probs = []\n",
    "test_targets_global = None\n",
    "\n",
    "for fold, (tr_rel, va_rel) in enumerate(skf.split(np.zeros(len(train_pool_idx)), y_train_pool), start=1):\n",
    "    tr_idx = train_pool_idx[tr_rel]\n",
    "    va_idx = train_pool_idx[va_rel]\n",
    "\n",
    "    train_loader, val_loader, test_loader = make_loaders_from_indices(\n",
    "        tr_idx, va_idx, test_idx,\n",
    "        batch_size=BATCH_SIZE, num_workers=2\n",
    "    )\n",
    "\n",
    "    model = make_model_resnet18(num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    run_name = f\"fold_{fold}_resnet18\"\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        mlflow.log_params({\n",
    "            \"dataset\": \"humpback-whale-identification\",\n",
    "            \"model\": \"resnet18\",\n",
    "            \"num_classes\": num_classes,\n",
    "            \"K\": K,\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"lr\": LR,\n",
    "            \"weight_decay\": WEIGHT_DECAY,\n",
    "            \"drop_new_whale\": DROP_NEW_WHALE,\n",
    "            \"min_samples_per_class\": MIN_SAMPLES_PER_CLASS,\n",
    "            \"top_n_classes\": TOP_N_CLASSES,\n",
    "            \"img_size\": IMG_SIZE,\n",
    "        })\n",
    "\n",
    "        for ep in range(1, EPOCHS + 1):\n",
    "            tr_loss, tr_acc, tr_map5, _, _ = run_epoch(\n",
    "                model, train_loader, criterion, optimizer)\n",
    "            va_loss, va_acc, va_map5, _, _ = run_epoch(\n",
    "                model, val_loader, criterion)\n",
    "\n",
    "            print(f\"Fold {fold} | Epoch {ep:02d} | \"\n",
    "                  f\"train: loss={tr_loss:.4f} acc={tr_acc:.4f} map5={tr_map5:.4f} | \"\n",
    "                  f\"val:   loss={va_loss:.4f} acc={va_acc:.4f} map5={va_map5:.4f}\")\n",
    "\n",
    "            mlflow.log_metrics({\n",
    "                \"train_loss\": tr_loss,\n",
    "                \"train_acc\": tr_acc,\n",
    "                \"train_map5\": tr_map5,\n",
    "                \"val_loss\": va_loss,\n",
    "                \"val_acc\": va_acc,\n",
    "                \"val_map5\": va_map5,\n",
    "            }, step=ep)\n",
    "\n",
    "        te_loss, te_acc, te_map5, te_probs, te_targets = run_epoch(\n",
    "            model, test_loader, criterion)\n",
    "        print(\n",
    "            f\"Fold {fold} TEST: loss={te_loss:.4f} acc={te_acc:.4f} map5={te_map5:.4f}\")\n",
    "\n",
    "        mlflow.log_metrics({\n",
    "            \"test_loss\": te_loss,\n",
    "            \"test_acc\": te_acc,\n",
    "            \"test_map5\": te_map5,\n",
    "        })\n",
    "\n",
    "        mlflow.pytorch.log_model(model, artifact_path=\"model\")\n",
    "\n",
    "        fold_test_probs.append(te_probs)\n",
    "        test_targets_global = te_targets\n",
    "\n",
    "        fold_rows.append({\n",
    "            \"fold\": fold,\n",
    "            \"val_acc_last\": va_acc,\n",
    "            \"val_map5_last\": va_map5,\n",
    "            \"test_acc\": te_acc,\n",
    "            \"test_map5\": te_map5,\n",
    "        })\n",
    "\n",
    "df_folds = pd.DataFrame(fold_rows)\n",
    "df_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a898c159",
   "metadata": {},
   "source": [
    "## 9) Mean-of-folds ensemble on TEST (average probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f25199",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_test_probs_arr = np.stack(fold_test_probs, axis=0)  # (K, Ntest, C)\n",
    "mean_probs = fold_test_probs_arr.mean(axis=0)              # (Ntest, C)\n",
    "\n",
    "mean_pred = mean_probs.argmax(axis=1)\n",
    "ensemble_acc = float(accuracy_score(test_targets_global, mean_pred))\n",
    "ensemble_map5 = map_at_k_from_probs(test_targets_global, mean_probs, k=5)\n",
    "\n",
    "print(\"Ensemble TEST acc :\", ensemble_acc)\n",
    "print(\"Ensemble TEST map5:\", ensemble_map5)\n",
    "\n",
    "with mlflow.start_run(run_name=\"ensemble_mean_of_folds\"):\n",
    "    mlflow.log_params({\n",
    "        \"dataset\": \"humpback-whale-identification\",\n",
    "        \"model\": \"resnet18\",\n",
    "        \"ensemble\": \"mean_probs\",\n",
    "        \"K\": K,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"lr\": LR,\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "        \"num_classes\": num_classes,\n",
    "    })\n",
    "    mlflow.log_metrics({\n",
    "        \"ensemble_test_acc\": ensemble_acc,\n",
    "        \"ensemble_test_map5\": ensemble_map5,\n",
    "    })\n",
    "\n",
    "df_compare = df_folds.copy()\n",
    "df_compare[\"ensemble_test_acc\"] = ensemble_acc\n",
    "df_compare[\"ensemble_test_map5\"] = ensemble_map5\n",
    "df_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2f68fb",
   "metadata": {},
   "source": [
    "## 10) Viewing MLflow results\n",
    "\n",
    "If you used `file:.../mlruns`, open a terminal in this notebook folder and run:\n",
    "\n",
    "```bash\n",
    "mlflow ui\n",
    "```\n",
    "\n",
    "Then open the printed local URL in your browser.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
